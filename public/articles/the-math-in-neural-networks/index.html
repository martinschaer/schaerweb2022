<!doctype html><html prefix="og: http://ogp.me/ns#" lang=en><head itemscope itemtype=https://schema.org/WebPage><title>The Math in Neural Networks - Martin Schaer Web</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,minimum-scale=1"><meta name=theme-color content="#00ff80"><meta property="og:title" content="The Math in Neural Networks"><meta property="og:description" content="These are quick notes about this topic that interests me. It&rsquo;s an evolving article, so bare with me while I gather my thoughts and stitch them all together.
From ChatGPT There is no single mathematical proof that neural networks work in general. However, there are many mathematical results that provide theoretical guarantees for the performance of neural networks under certain conditions.
One of the key results in the theory of neural networks is the universal approximation theorem, which states that a feedforward neural network with a single hidden layer containing a sufficient number of units can approximate any continuous function to arbitrary accuracy."><meta property="og:type" content="article"><meta property="og:url" content="https://www.schaerweb.com/articles/the-math-in-neural-networks/"><meta property="og:image" content="https://www.schaerweb.com/articles/f1-3d/008.png"><meta property="article:section" content="articles"><meta property="article:published_time" content="2022-12-16T16:30:13-06:00"><meta property="article:modified_time" content="2022-12-16T17:09:35-06:00"><meta property="og:site_name" content="Martin Schaer Web"><meta itemprop=name content="The Math in Neural Networks"><meta itemprop=description content="These are quick notes about this topic that interests me. It&rsquo;s an evolving article, so bare with me while I gather my thoughts and stitch them all together.
From ChatGPT There is no single mathematical proof that neural networks work in general. However, there are many mathematical results that provide theoretical guarantees for the performance of neural networks under certain conditions.
One of the key results in the theory of neural networks is the universal approximation theorem, which states that a feedforward neural network with a single hidden layer containing a sufficient number of units can approximate any continuous function to arbitrary accuracy."><meta itemprop=datePublished content="2022-12-16T16:30:13-06:00"><meta itemprop=dateModified content="2022-12-16T17:09:35-06:00"><meta itemprop=wordCount content="269"><meta itemprop=image content="https://www.schaerweb.com/articles/f1-3d/008.png"><meta itemprop=keywords content="AI,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.schaerweb.com/articles/f1-3d/008.png"><meta name=twitter:title content="The Math in Neural Networks"><meta name=twitter:description content="These are quick notes about this topic that interests me. It&rsquo;s an evolving article, so bare with me while I gather my thoughts and stitch them all together.
From ChatGPT There is no single mathematical proof that neural networks work in general. However, there are many mathematical results that provide theoretical guarantees for the performance of neural networks under certain conditions.
One of the key results in the theory of neural networks is the universal approximation theorem, which states that a feedforward neural network with a single hidden layer containing a sufficient number of units can approximate any continuous function to arbitrary accuracy."><script type=module src=https://www.schaerweb.com/main.js defer></script>
<link rel=icon href=https://www.schaerweb.com/favicon.svg type=image/svg+xml><link rel=stylesheet href=https://www.schaerweb.com/normalize.css><link rel=stylesheet href=https://www.schaerweb.com/styles/main.min.css></head><body><schaerweb-titlebar class=print:hidden><nav slot=nav><a href=https://www.schaerweb.com/articles/ title=Articles target>Articles</a>
<a href=https://www.schaerweb.com/about/ title=About target>About</a>
<a href=https://github.com/martinschaer title=GitHub target=blank>GitHub</a>
<a href=https://www.schaerweb.com/about/#social-links title=Social target>Social</a></nav></schaerweb-titlebar><div id=app><div class=paper><article class=article><p class="my-0 small text-muted print:hidden">Back to <a href=https://www.schaerweb.com/articles/>Articles</a></p><header><h1>The Math in Neural Networks</h1></header><div class=article__metadata><ul class=metadata><li><span>Tags:</span>
<a href=https://www.schaerweb.com/tags/ai/>AI</a></li><li><span>Last modified:</span>
<schaerweb-date unix=1671232175>Friday, December 16, 2022, 5:09pm CST</schaerweb-date></li></ul></div><main class=article__content><p>These are quick notes about this topic that interests me. It&rsquo;s an evolving article, so bare with me while I gather my thoughts and stitch them all together.</p><h2 id=from-chatgpthttpschatopenaicomchat>From <a href=https://chat.openai.com/chat>ChatGPT</a></h2><p>There is no single mathematical proof that neural networks work in general. However, there are many mathematical results that provide theoretical guarantees for the performance of neural networks under certain conditions.</p><p>One of the key results in the theory of neural networks is the universal approximation theorem, which states that a feedforward neural network with a single hidden layer containing a sufficient number of units can approximate any continuous function to arbitrary accuracy. This theorem provides a theoretical guarantee for the ability of neural networks to learn a wide range of functions, but it does not guarantee that a particular neural network architecture or training algorithm will be able to successfully learn a particular function in practice.</p><p>Other mathematical results provide guarantees for the convergence of specific training algorithms for neural networks under certain conditions, such as the convergence of stochastic gradient descent for training feedforward neural networks with smooth loss functions. These results can provide theoretical support for the effectiveness of specific training techniques, but they do not necessarily guarantee the success of these techniques in all cases.</p><p>Overall, the mathematical theory of neural networks is an active area of research, and there are many open questions and challenges that remain to be addressed.</p><h2 id=from-the-age-of-spiritual-machines-by-ray-kurzweil>From The Age of Spiritual Machines by Ray Kurzweil</h2><p>Get cite from the book, where Ray states that you need at least 60% of accuracy in a training set to train a neural network.</p></main></article></div></div><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-15165899-1","auto"),ga("send","pageview"))</script></body></html>